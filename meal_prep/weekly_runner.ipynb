{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start by getting pandas, regex, and numpy\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import helper as helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background # \n",
    "This is the stuff we need to make this work. </br>\n",
    "So we need the relevant dataframes, and the relevant functions. </br>\n",
    "The functions are: \n",
    "1) 'ingredient_divider' which takes a string and returns the percengtage of ingredietns which are covered\n",
    "2) 'matching_ingredient' which takes a url, and updates the recipes dataframe with the ingredients and the percentage of ingredients covered\n",
    "3) 'matching_ingredient_df' which takes a df, and updates the recipes dataframe with the ingredients and the percentage of ingredients covered\n",
    "4) 'check_fridge_availability' which takes an ingredient name and amount needed, and returns the amount available in the fridge\n",
    "5) 'calculate_cost_and_remaining' which takes a row and returns the cost and remaining amount of ingredients needed\n",
    "6) 'calculate_cost_of_urls' which takes a list of urls and returns the total cost of the ingredients needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv('recipes.csv')\n",
    "ingredients_df = pd.read_csv('ingredients_df.csv')\n",
    "pantry = pd.read_csv('pantry.csv')\n",
    "# lets go to the download folder and import the ingredients_3.csv file\n",
    "conversion = pd.read_csv('conversion.csv')\n",
    "fridge = pd.read_csv('fridge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients</th>\n",
       "      <th>amounts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>løg</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hvidløg</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rødvin</td>\n",
       "      <td>740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>olivenolie</td>\n",
       "      <td>970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>grøntsagsbouillon</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>soja</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mango</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rødløg</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jalapenos</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lime</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>majs</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cheddar ost</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ingredients  amounts\n",
       "0                 løg      3.0\n",
       "1             hvidløg      8.0\n",
       "2              rødvin    740.0\n",
       "3          olivenolie    970.0\n",
       "8   grøntsagsbouillon    150.0\n",
       "9                soja    200.0\n",
       "11              mango    125.0\n",
       "12             rødløg      6.5\n",
       "13          jalapenos     45.0\n",
       "15               lime      4.0\n",
       "16               majs    250.0\n",
       "18        cheddar ost     75.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fridge = pd.read_clipboard()\n",
    "fridge.columns = ['ingredients', 'amounts']\n",
    "fridge = fridge[fridge.amounts > 0]\n",
    "\n",
    "fridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridge.loc[len(fridge)] = ['æg', 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets merge the purchase_unit and the veggie average on to the fridge \n",
    "fridge = pd.merge(fridge, ingredients_df[['ingredients', 'purchase_unit', 'veggie_average']], on='ingredients', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets copy all this in to a clipboard where we can base into a text emssage, where we send the amount we have left, the veggeige average followed by the purchase unit and then the ingredient\n",
    "fridge['text'] = fridge['amounts'].astype(str) + ', tilbage ' + round(fridge['veggie_average'], 2).astype(str) + ', normalt ' + fridge['purchase_unit'].astype(str) + ' ' + fridge['ingredients'].astype(str)\n",
    "# now lets copy all that into one string\n",
    "fridge_text = fridge['text'].str.cat(sep='\\n')\n",
    "fridge_text.replace('nan', 'ved ikke')\n",
    "import pyperclip    \n",
    "pyperclip.copy(fridge_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing URLs currently found 43, and have 233 to go:   7%|▋         | 70/1000 [00:52<10:25,  1.49url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 55, and have 247 to go:   9%|▉         | 92/1000 [01:08<11:07,  1.36url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 111, and have 342 to go:  19%|█▉        | 192/1000 [02:29<12:35,  1.07url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 114, and have 347 to go:  20%|██        | 200/1000 [02:34<09:58,  1.34url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 144, and have 378 to go:  25%|██▌       | 251/1000 [03:15<09:26,  1.32url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 162, and have 406 to go:  28%|██▊       | 281/1000 [03:39<09:20,  1.28url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 171, and have 414 to go:  30%|██▉       | 296/1000 [03:51<10:48,  1.09url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 180, and have 415 to go:  31%|███▏      | 313/1000 [04:02<08:22,  1.37url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 193, and have 414 to go:  34%|███▍      | 338/1000 [04:31<13:35,  1.23s/url]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 195, and have 413 to go:  34%|███▍      | 341/1000 [04:33<10:18,  1.07url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 208, and have 417 to go:  37%|███▋      | 366/1000 [04:53<07:52,  1.34url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 210, and have 417 to go:  37%|███▋      | 368/1000 [04:54<06:52,  1.53url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 212, and have 415 to go:  37%|███▋      | 372/1000 [04:57<06:43,  1.56url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 218, and have 413 to go:  38%|███▊      | 383/1000 [05:06<10:09,  1.01url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 219, and have 412 to go:  38%|███▊      | 384/1000 [05:06<07:28,  1.37url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 226, and have 411 to go:  40%|███▉      | 397/1000 [05:19<10:46,  1.07s/url]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 231, and have 408 to go:  40%|████      | 405/1000 [05:24<07:25,  1.34url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 261, and have 406 to go:  46%|████▋     | 465/1000 [06:11<06:02,  1.48url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 286, and have 409 to go:  51%|█████▏    | 513/1000 [06:50<06:18,  1.29url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 301, and have 402 to go:  54%|█████▍    | 543/1000 [07:12<06:12,  1.23url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 307, and have 400 to go:  55%|█████▌    | 551/1000 [07:17<04:56,  1.52url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 346, and have 388 to go:  64%|██████▎   | 636/1000 [08:22<04:22,  1.38url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 355, and have 382 to go:  65%|██████▌   | 653/1000 [08:34<04:53,  1.18url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 385, and have 361 to go:  71%|███████▏  | 714/1000 [09:26<03:55,  1.21url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 389, and have 361 to go:  72%|███████▏  | 720/1000 [09:30<03:41,  1.27url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 413, and have 350 to go:  78%|███████▊  | 777/1000 [10:13<02:59,  1.25url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 422, and have 348 to go:  79%|███████▉  | 790/1000 [10:21<02:21,  1.48url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 434, and have 334 to go:  82%|████████▏ | 816/1000 [10:38<02:11,  1.40url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 443, and have 340 to go:  83%|████████▎ | 828/1000 [10:47<01:59,  1.44url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 445, and have 334 to go:  84%|████████▎ | 836/1000 [10:52<01:42,  1.60url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 459, and have 321 to go:  86%|████████▋ | 865/1000 [11:11<01:36,  1.40url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 465, and have 315 to go:  88%|████████▊ | 877/1000 [11:19<01:30,  1.35url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 473, and have 316 to go:  89%|████████▉ | 893/1000 [11:30<01:30,  1.18url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 509, and have 299 to go:  96%|█████████▌| 962/1000 [12:25<00:31,  1.21url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 531, and have 291 to go: : 1003url [12:56,  1.46url/s]                       Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 533, and have 284 to go: : 1011url [13:01,  1.37url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 551, and have 275 to go: : 1044url [13:25,  1.41url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 568, and have 265 to go: : 1078url [13:51,  1.58url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 601, and have 265 to go: : 1135url [14:40,  1.31url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 603, and have 262 to go: : 1139url [14:42,  1.65url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 604, and have 260 to go: : 1141url [14:43,  1.61url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 606, and have 261 to go: : 1143url [14:44,  1.51url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 625, and have 261 to go: : 1176url [15:11,  1.23url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 634, and have 253 to go: : 1195url [15:26,  1.34url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 644, and have 248 to go: : 1215url [15:43,  1.40url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 674, and have 224 to go: : 1279url [16:33,  1.20url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 687, and have 223 to go: : 1297url [16:46,  1.23url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 688, and have 222 to go: : 1298url [16:47,  1.46url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 697, and have 213 to go: : 1317url [17:02,  1.19url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 705, and have 207 to go: : 1334url [17:18,  1.08url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 717, and have 199 to go: : 1358url [17:37,  1.55url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 722, and have 196 to go: : 1365url [17:42,  1.12url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 725, and have 192 to go: : 1373url [17:48,  1.20url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 729, and have 185 to go: : 1383url [17:55,  1.25url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 743, and have 178 to go: : 1408url [18:15,  1.19url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 774, and have 144 to go: : 1476url [19:06,  1.40url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 782, and have 137 to go: : 1492url [19:21,  1.02url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 788, and have 130 to go: : 1504url [19:31,  1.14url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 795, and have 123 to go: : 1517url [19:41,  1.21url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 799, and have 119 to go: : 1524url [19:48,  1.15url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 800, and have 118 to go: : 1525url [19:48,  1.46url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 807, and have 111 to go: : 1538url [19:58,  1.33url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 820, and have 100 to go: : 1562url [20:18,  1.24url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 827, and have 93 to go: : 1576url [20:29,  1.13url/s] Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 832, and have 91 to go: : 1583url [20:35,  1.12url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 849, and have 77 to go: : 1619url [21:07,  1.25s/url]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 852, and have 72 to go: : 1626url [21:12,  1.32url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 859, and have 69 to go: : 1638url [21:22,  1.09url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 861, and have 67 to go: : 1641url [21:24,  1.17url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 865, and have 62 to go: : 1649url [21:30,  1.09url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 901, and have 21 to go: : 1728url [22:38,  1.31url/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Analyzing URLs currently found 918, and have 1 to go: : 1765url [23:10,  1.27url/s] \n",
      "100%|██████████| 918/918 [00:00<00:00, 506627.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def urls_on_same_domain(url, max_urls=400):\n",
    "    \"\"\"\n",
    "    Counts the number of unique URLs on the same domain as the given URL,\n",
    "    iterating through links until no more are found or a max limit is reached.\n",
    "    The progress is displayed using tqdm.\n",
    "\n",
    "    Args:\n",
    "    url (str): The starting URL to fetch and analyze.\n",
    "    max_urls (int): Maximum number of URLs to fetch.\n",
    "\n",
    "    Returns:\n",
    "    int: The total number of unique URLs found on the same domain.\n",
    "    \"\"\"\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = set([url])\n",
    "    domain = urlparse(url).netloc\n",
    "\n",
    "    # Common user-agent string of a web browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    with tqdm(total=max_urls, desc=f\"Analyzing URLs currently found {len(visited_urls)}, and have {len(urls_to_visit)} to go\", unit=\"url\") as pbar:\n",
    "        while len(urls_to_visit) != 0 and len(visited_urls) < max_urls:\n",
    "            current_url = urls_to_visit.pop()\n",
    "            try:\n",
    "                response = requests.get(current_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                anchors = soup.find_all('a', href=True)\n",
    "\n",
    "                for anchor in anchors:\n",
    "                    href = anchor['href']\n",
    "                    full_url = urljoin(current_url, href)\n",
    "                    href_domain = urlparse(full_url).netloc\n",
    "\n",
    "                    if href_domain == domain and full_url not in visited_urls and '#comment' not in full_url and '#respond' not in full_url:\n",
    "                        urls_to_visit.add(full_url)\n",
    "\n",
    "                visited_urls.add(current_url)\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"Analyzing URLs currently found {len(visited_urls)}, and have {len(urls_to_visit)} to go\")\n",
    "            except requests.RequestException:\n",
    "                # Handle exceptions for requests\n",
    "                continue\n",
    "\n",
    "    return visited_urls\n",
    "\n",
    "def get_text_from_url(url):\n",
    "    \"\"\"\n",
    "    Fetches the content from a given URL and returns the text separated by lines.\n",
    "    If the URL gets redirected more than twice, returns 'redirect link'.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL from which to fetch the content.\n",
    "\n",
    "    Returns:\n",
    "    str: A string containing the text from the URL, separated by lines, or 'redirect link'.\n",
    "    \"\"\"\n",
    "    # Common user-agent string of a web browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL with the user-agent header and redirection allowed\n",
    "    response = requests.get(url, headers=headers, allow_redirects=True)\n",
    "    \n",
    "    # Check the number of redirects\n",
    "    if len(response.history) > 2:\n",
    "        return 'redirect link'\n",
    "    \n",
    "    # Raise an exception if the request was unsuccessful\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract and return the text, separated by lines\n",
    "    return '\\n'.join(soup.stripped_strings)\n",
    "\n",
    "url = 'https://vegetariskhverdag.dk/opskrifter/'\n",
    "\n",
    "test_urls = urls_on_same_domain(url, max_urls=1000)\n",
    "\n",
    "# now lets write them to a file called 'dovne_vegetar.txt'\n",
    "with open('dovne_vegetar.txt', 'w') as f:\n",
    "    for url in tqdm(test_urls):\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
